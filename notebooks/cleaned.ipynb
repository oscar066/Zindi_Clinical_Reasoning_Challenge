{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b9a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /teamspace/studios/this_studio/.conda\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m15 packages\u001b[0m \u001b[2min 125ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/288.00 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 14.92 KiB/288.00 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 30.92 KiB/288.00 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 46.92 KiB/288.00 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 62.92 KiB/288.00 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 76.71 KiB/288.00 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 92.71 KiB/288.00 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 108.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 124.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 140.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)m-------------\u001b[0m\u001b[0m 156.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)2m------------\u001b[0m\u001b[0m 172.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\u001b[2m----------\u001b[0m\u001b[0m 188.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--\u001b[2m--------\u001b[0m\u001b[0m 204.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---\u001b[2m-------\u001b[0m\u001b[0m 220.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-----\u001b[2m-----\u001b[0m\u001b[0m 236.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------\u001b[2m---\u001b[0m\u001b[0m 252.71 KiB/288.00 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m                                                   \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mseaborn\u001b[0m\u001b[2m==0.13.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac87656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  master_index       county                            health_level  \\\n",
      "0     ID_VBWWP  uasin gishu  sub county hospitals and nursing homes   \n",
      "1     ID_XMBBY  uasin gishu             national referral hospitals   \n",
      "2     ID_JZNZW       kiambu  sub county hospitals and nursing homes   \n",
      "3     ID_QOQTK  uasin gishu             national referral hospitals   \n",
      "4     ID_ZFJBM  uasin gishu             national referral hospitals   \n",
      "\n",
      "   years_of_experience                                             prompt  \\\n",
      "0                 18.0  i am a nurse with 18 years of experience in ge...   \n",
      "1                 17.0  i am a nurse with 17 years of experience in ge...   \n",
      "2                 12.0  i am a nurse with 12 years of experience in ge...   \n",
      "3                 12.0  i am a nurse with 12 years of experience in pr...   \n",
      "4                 16.0  i am a nurse with 16 years of experience in ge...   \n",
      "\n",
      "          nursing_competency     clinical_panel  \\\n",
      "0  pediatric emergency burns            surgery   \n",
      "1               child health        paediatrics   \n",
      "2          general emergency  internal medicine   \n",
      "3              critical care  internal medicine   \n",
      "4               adult health  internal medicine   \n",
      "\n",
      "                                           clinician  \\\n",
      "0  summary a 4 year old with 5 superficial burns ...   \n",
      "1  summary 6 year old present with vomiting and a...   \n",
      "2  summary a 47 year old man presents with severe...   \n",
      "3  summary 72 year old female with inability to w...   \n",
      "4  a 22 year old female presents with headache di...   \n",
      "\n",
      "                                              gpt4.0  \\\n",
      "0  given your vast experience as a nurse in uasin...   \n",
      "1  clinical summary • a 6 year old girl with know...   \n",
      "2  in this case you re dealing with a 47 year old...   \n",
      "3  given er s clinical presentation and vitals th...   \n",
      "4  the 22 year old female patient is presenting w...   \n",
      "\n",
      "                                               llama  \\\n",
      "0  1 immediate treatment protocol for second degr...   \n",
      "1  based on the symptoms and signs you ve describ...   \n",
      "2  firstly i must commend you on your thorough hi...   \n",
      "3  to me with this query based on the information...   \n",
      "4  thank you for presenting this case based on th...   \n",
      "\n",
      "                                              gemini  \\\n",
      "0  here s a response addressing the questions reg...   \n",
      "1  based on the presentation the 6 year old girl ...   \n",
      "2  this 47 year old male presenting with severe r...   \n",
      "3  this 92 year old female patient er presents wi...   \n",
      "4  this 22 year old female patient presents with ...   \n",
      "\n",
      "                                          ddx_snomed  \n",
      "0  288514009 burn involving 5 percent of body sur...  \n",
      "1  420270002 ketoacidosis due to type 1 diabetes ...  \n",
      "2  13200003 peptic ulcer disorder 25458004 acute ...  \n",
      "3  14760008 constipation finding 419284004 altere...  \n",
      "4  95874006 carbon monoxide poisoning from fire d...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('/teamspace/studios/this_studio/Zindi_Clinical_Reasoning_Challenge/data/kenya-clinical-reasoning-challenge20250407-27832-p30dso/cleaned_train.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # show all columns\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# print(f\"Dataset Shape: {df.shape}\")\n",
    "\n",
    "# print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# print(df.info())\n",
    "\n",
    "# print(df.describe(include='all'))\n",
    "\n",
    "# missing_values = df.isnull().sum()\n",
    "# missing_values = missing_values[missing_values > 0]\n",
    "# print(\"Missing values:\\n\", missing_values)\n",
    "\n",
    "# import missingno as msno\n",
    "# msno.matrix(df)\n",
    "# plt.show()\n",
    "\n",
    "# # Check if 'target' column exists\n",
    "# if 'target' in df.columns:\n",
    "#     print(df['target'].value_counts())\n",
    "#     sns.countplot(data=df, x='target')\n",
    "#     plt.title('Target Variable Distribution')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e3824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns: ['master_index', 'county', 'health_level', 'prompt', 'nursing_competency', 'clinical_panel', 'clinician', 'gpt4.0', 'llama', 'gemini', 'ddx_snomed']\n",
      "Numerical Columns: ['years_of_experience']\n"
     ]
    }
   ],
   "source": [
    "# Separate categorical and numerical features\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical Columns: {categorical_cols}\")\n",
    "print(f\"Numerical Columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for numerical features\n",
    "df[numerical_cols].hist(figsize=(15, 10), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for categorical features\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=df, x=col, order=df[col].value_counts().index)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix to show correlation between numerical variables (bivariate analysis)\n",
    "corr = df[numerical_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Detection\n",
    "# Boxplots for numerical variables\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(data=df, x=col)\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature-Target Relationships\n",
    "# If 'target' exists\n",
    "if 'target' in df.columns:\n",
    "    for col in numerical_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.boxplot(x='target', y=col, data=df)\n",
    "        plt.title(f'{col} vs Target')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Features vs Target\n",
    "# If 'target' exists\n",
    "if 'target' in df.columns:\n",
    "    for col in categorical_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.countplot(x=col, hue='target', data=df)\n",
    "        plt.title(f'{col} vs Target')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "#Summary Observations\n",
    "# Display some basic summary\n",
    "print(\"Summary Observations:\")\n",
    "print(\"- Dataset shape:\", df.shape)\n",
    "print(\"- Number of missing values:\", missing_values.sum())\n",
    "print(\"- Numerical features:\", len(numerical_cols))\n",
    "print(\"- Categorical features:\", len(categorical_cols))\n",
    "print(\"- Target distribution (if exists):\")\n",
    "if 'target' in df.columns:\n",
    "    print(df['target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d0a796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0b10da922d40c98400155d7e460b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0e4dd024a24743b1b11607c6ae6536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc531b1447f443f917a125fcbdbdc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24d79908f624c84bcea2736088a5cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070be03e7c024034816336c403c7d19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Choose a model and tokenizer\n",
    "model_name = \"distilgpt2\"  # Lightweight, good for CPU training\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Add padding token if missing (GPT-2 has none by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Split into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"prompt\"].tolist(),\n",
    "    df[\"clinician\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Define a custom dataset class\n",
    "class ClinicalReasoningDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_length=512):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        labels = target_encoding[\"input_ids\"].squeeze()\n",
    "\n",
    "        # Replace padding token id's of the labels by -100 so they are ignored by the loss\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# 5. Create datasets\n",
    "train_dataset = ClinicalReasoningDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = ClinicalReasoningDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ea77132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Add pad token for GPT-2 compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ae8ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb33c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486c7680c2db4b66ade5fa5f7c429038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cb05885ce747839b32fc6fe6a87d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5412/2235743597.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 00:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.177000</td>\n",
       "      <td>6.984604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.920800</td>\n",
       "      <td>6.905922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.789100</td>\n",
       "      <td>6.896348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=360, training_loss=7.080420345730252, metrics={'train_runtime': 50.8212, 'train_samples_per_second': 14.167, 'train_steps_per_second': 7.084, 'total_flos': 94066830213120.0, 'train_loss': 7.080420345730252, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# 1. Load the base model for causal language modeling (text generation)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust for any added tokens\n",
    "\n",
    "# 2. Define training arguments (tuned for CPU)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_clinical_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,         # Disable huggingface hub push\n",
    "    fp16=False,                # CPU only\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 3. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 4. Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1856aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate model outputs\n",
    "def generate_prediction(prompt, model, tokenizer, max_input_length=200, max_output_length=250):\n",
    "    # Tokenize the input and truncate if too long\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_output_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c3725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6baa2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply to dataset\n",
    "df[\"prediction\"] = df[\"prompt\"].apply(lambda x: generate_prediction(x, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c0ccbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      i am a nurse with 18 years of experience in ge...\n",
       "1      i am a nurse with 17 years of experience in ge...\n",
       "2      i am a nurse with 12 years of experience in ge...\n",
       "3      i am a nurse with 12 years of experience in pr...\n",
       "4      i am a nurse with 16 years of experience in ge...\n",
       "                             ...                        \n",
       "295    i am a nurse with 12 years of experience in ge...\n",
       "296    i am a nurse with 22 years of experience in ge...\n",
       "297    i am a nurse with 7 years of experience in gen...\n",
       "298    i am a nurse with 15 years of experience in pr...\n",
       "299    i am a nurse with 18 years of experience in ge...\n",
       "Name: prediction, Length: 300, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a66e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df[[\"master_index\", \"prediction\"]].rename(columns={\n",
    "    \"master_index\": \"ID\",\n",
    "    \"prediction\": \"Prediction\"\n",
    "})\n",
    "\n",
    "submission.to_csv(\"zindi_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50608be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc81cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"Prediction\"] = test_df[\"Prompt\"].apply(lambda x: generate_prediction(x, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df[[\"Master_Index\", \"Prediction\"]].rename(columns={\n",
    "    \"Master_Index\": \"ID\"\n",
    "})\n",
    "submission_df.to_csv(\"zindi_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"cleaned_train.csv\")\n",
    "\n",
    "# Use prediction column or fallback\n",
    "if \"prediction\" not in df.columns:\n",
    "    df[\"prediction\"] = df[\"gpt4.0\"]\n",
    "\n",
    "# BLEU score calculation\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_scores = [\n",
    "    sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
    "    for ref, pred in zip(df[\"clinician\"], df[\"prediction\"])\n",
    "]\n",
    "\n",
    "# Compute average BLEU\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Average BLEU Score: {round(avg_bleu, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba72c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load your dataset with ground truth and predictions\n",
    "df = pd.read_csv(\"cleaned_train.csv\")  # Adjust path if needed\n",
    "\n",
    "# Use model predictions\n",
    "if \"prediction\" not in df.columns:\n",
    "    df[\"prediction\"] = df[\"gpt4.0\"]  # fallback if custom predictions are not present\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE-L F1 scores for each sample\n",
    "rouge_l_scores = [\n",
    "    scorer.score(ref, pred)[\"rougeL\"].fmeasure\n",
    "    for ref, pred in zip(df[\"clinician\"], df[\"prediction\"])\n",
    "]\n",
    "\n",
    "# Calculate average ROUGE-L score\n",
    "avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "print(f\"Average ROUGE-L F1 Score: {round(avg_rouge_l, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb665ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
